#!/usr/bin/env python3
"""
Import VAT corpus ZIP files into Uderia using the Python API directly.

Reads the ZIP files produced by import_vat_corpus.py and creates knowledge
repositories by writing directly to the database and ChromaDB â€” bypassing
the REST API to avoid body size limits.

Usage:
    python agent_packs/vat/import_vat_to_uderia.py [--base-url http://localhost:5050]

Prerequisites:
    - Uderia server running (for authentication and user lookup)
    - ZIP files already generated by import_vat_corpus.py
    - pip install -e . (Uderia must be installed in editable mode)
"""

import argparse
import json
import sqlite3
import sys
import time
import zipfile
from datetime import datetime, timezone
from pathlib import Path

import chromadb
from chromadb.utils import embedding_functions
import requests


CHROMA_PERSIST_DIR = ".chromadb_rag_cache"
AUTH_DB = "tda_auth.db"


def get_jwt_and_user(base_url: str, username: str, password: str) -> tuple:
    """Authenticate and return (JWT token, user_uuid)."""
    resp = requests.post(
        f"{base_url}/api/v1/auth/login",
        json={"username": username, "password": password},
        timeout=30,
    )
    if resp.status_code != 200:
        print(f"ERROR: Login failed ({resp.status_code}): {resp.text}")
        sys.exit(1)
    data = resp.json()
    token = data.get("token")
    user_uuid = data.get("user_id") or data.get("user_uuid")

    if not user_uuid:
        # Fall back to database lookup
        conn = sqlite3.connect(AUTH_DB)
        c = conn.cursor()
        c.execute("SELECT id FROM users WHERE username = ?", (username,))
        row = c.fetchone()
        conn.close()
        if row:
            user_uuid = row[0]
        else:
            print("ERROR: Could not determine user UUID")
            sys.exit(1)

    return token, user_uuid


def create_collection_in_db(conn: sqlite3.Connection, name: str, collection_name: str,
                             description: str, user_uuid: str, embedding_model: str,
                             chunking_strategy: str, chunk_size: int, chunk_overlap: int) -> int:
    """Create a collection record in tda_auth.db and return the collection_id."""
    cursor = conn.cursor()
    now = datetime.now(timezone.utc).isoformat()

    cursor.execute("""
        INSERT INTO collections
        (name, collection_name, owner_user_id, repository_type, mcp_server_id,
         enabled, created_at, description, visibility, is_marketplace_listed,
         subscriber_count, marketplace_category, marketplace_tags, marketplace_long_description,
         chunking_strategy, chunk_size, chunk_overlap, embedding_model)
        VALUES (?, ?, ?, 'knowledge', '',
                1, ?, ?, 'private', 0,
                0, '', '', '',
                ?, ?, ?, ?)
    """, (name, collection_name, user_uuid,
          now, description,
          chunking_strategy, chunk_size, chunk_overlap, embedding_model))

    conn.commit()
    return cursor.lastrowid


def import_zip(zip_path: Path, chroma_client: chromadb.PersistentClient,
               db_conn: sqlite3.Connection, user_uuid: str) -> dict:
    """Import a single ZIP file into Uderia."""
    corpus_name = zip_path.stem.replace("_Corpus_import", "")

    # Extract ZIP
    import tempfile
    with tempfile.TemporaryDirectory() as tmpdir:
        with zipfile.ZipFile(zip_path, 'r') as zf:
            zf.extractall(tmpdir)

        # Read metadata
        with open(Path(tmpdir) / "collection_metadata.json") as f:
            metadata = json.load(f)

        # Read documents
        with open(Path(tmpdir) / "documents.json") as f:
            documents_data = json.load(f)

    ids = documents_data.get("ids", [])
    documents = documents_data.get("documents", [])
    metadatas = documents_data.get("metadatas", [])
    embeddings = documents_data.get("embeddings", [])

    if not ids:
        return {"status": "error", "message": "No documents in ZIP"}

    # Generate unique collection name for ChromaDB
    collection_name = f"col_{user_uuid}_{int(time.time())}_{corpus_name.lower()}"
    display_name = metadata.get("name", f"VAT {corpus_name}")
    description = metadata.get("description", "")
    embedding_model = metadata.get("embedding_model", "all-MiniLM-L6-v2")
    chunking_strategy = metadata.get("chunking_strategy", "recursive")
    chunk_size = metadata.get("chunk_size", 1000)
    chunk_overlap = metadata.get("chunk_overlap", 200)

    # Step 1: Create database record
    collection_id = create_collection_in_db(
        db_conn, display_name, collection_name, description, user_uuid,
        embedding_model, chunking_strategy, chunk_size, chunk_overlap,
    )

    # Step 2: Create ChromaDB collection
    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=embedding_model
    )

    chroma_collection = chroma_client.create_collection(
        name=collection_name,
        embedding_function=embedding_func,
        metadata={"hnsw:space": "cosine", "repository_type": "knowledge"},
    )

    # Step 3: Add documents in batches (ChromaDB has a batch size limit)
    BATCH_SIZE = 5000
    total_added = 0

    for start in range(0, len(ids), BATCH_SIZE):
        end = min(start + BATCH_SIZE, len(ids))
        batch_ids = ids[start:end]
        batch_docs = documents[start:end]
        batch_metas = metadatas[start:end] if metadatas else None
        batch_embeds = embeddings[start:end] if embeddings else None

        # Sanitize metadata: ChromaDB requires flat values, no None
        if batch_metas:
            for meta in batch_metas:
                for key in list(meta.keys()):
                    if meta[key] is None:
                        meta[key] = ""
                # Ensure collection_id is set
                meta["collection_id"] = collection_id
                meta["repository_type"] = "knowledge"

        chroma_collection.add(
            ids=batch_ids,
            documents=batch_docs,
            metadatas=batch_metas,
            embeddings=batch_embeds,
        )
        total_added += len(batch_ids)

    # Step 4: Register unique documents in knowledge_documents table
    seen_doc_ids = set()
    cursor = db_conn.cursor()
    now = datetime.now(timezone.utc).isoformat()

    for meta in (metadatas or []):
        doc_id = meta.get("document_id", "")
        if doc_id and doc_id not in seen_doc_ids:
            seen_doc_ids.add(doc_id)
            cursor.execute("""
                INSERT OR IGNORE INTO knowledge_documents
                (collection_id, document_id, filename, document_type, title,
                 author, source, category, tags, file_size, content_hash, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                collection_id,
                doc_id,
                meta.get("filename", ""),
                meta.get("document_type", "json"),
                meta.get("title", meta.get("filename", "")),
                meta.get("author", ""),
                "import",
                meta.get("category", corpus_name),
                meta.get("tags", f"vat_import,{corpus_name}"),
                0,
                "",
                now,
            ))

    db_conn.commit()

    return {
        "status": "success",
        "collection_id": collection_id,
        "collection_name": display_name,
        "chroma_name": collection_name,
        "chunks": total_added,
        "documents": len(seen_doc_ids),
    }


def main():
    parser = argparse.ArgumentParser(description="Import VAT corpus ZIPs into Uderia (Python API)")
    parser.add_argument("--base-url", type=str, default="http://localhost:5050")
    parser.add_argument("--username", type=str, default="admin")
    parser.add_argument("--password", type=str, default="admin")
    parser.add_argument(
        "--import-dir", type=str,
        default=str(Path(__file__).parent / "import_output"),
    )
    args = parser.parse_args()

    import_dir = Path(args.import_dir)
    if not import_dir.exists():
        print(f"ERROR: Import directory not found: {import_dir}")
        sys.exit(1)

    zip_files = sorted(import_dir.glob("*_Corpus_import.zip"))
    if not zip_files:
        print(f"ERROR: No *_Corpus_import.zip files found in {import_dir}")
        sys.exit(1)

    print(f"Found {len(zip_files)} ZIP files to import")

    # Authenticate to get user UUID
    print(f"Authenticating as '{args.username}'...")
    _token, user_uuid = get_jwt_and_user(args.base_url, args.username, args.password)
    print(f"User UUID: {user_uuid}")

    # Initialize ChromaDB client
    chroma_dir = Path(CHROMA_PERSIST_DIR)
    chroma_dir.mkdir(parents=True, exist_ok=True)
    chroma_client = chromadb.PersistentClient(path=str(chroma_dir))
    print(f"ChromaDB: {chroma_dir}")

    # Open database connection
    db_conn = sqlite3.connect(AUTH_DB)

    # Import each ZIP
    mapping = {}
    for zip_path in zip_files:
        corpus_key = zip_path.stem.replace("_Corpus_import", "_Corpus")
        print(f"\nImporting: {zip_path.name} ...", end=" ", flush=True)

        result = import_zip(zip_path, chroma_client, db_conn, user_uuid)

        if result["status"] == "success":
            print(f"OK (id={result['collection_id']}, {result['documents']} docs, {result['chunks']} chunks)")
            mapping[corpus_key] = {
                "collection_id": result["collection_id"],
                "collection_name": result["collection_name"],
                "chroma_name": result["chroma_name"],
                "chunks": result["chunks"],
                "documents": result["documents"],
            }
        else:
            print(f"FAILED: {result['message']}")
            mapping[corpus_key] = {"error": result["message"]}

    db_conn.close()

    # Save mapping
    mapping_path = import_dir / "collection_mapping.json"
    with open(mapping_path, 'w') as f:
        json.dump(mapping, f, indent=2)

    # Summary
    print(f"\n{'='*60}")
    print("IMPORT SUMMARY")
    print(f"{'='*60}")
    ok = 0
    for corpus, info in mapping.items():
        if "error" in info:
            print(f"  FAIL  {corpus}: {info['error'][:80]}")
        else:
            print(f"  OK    {corpus}: id={info['collection_id']}, {info['documents']} docs, {info['chunks']} chunks")
            ok += 1

    print(f"\n{ok}/{len(mapping)} imported successfully")
    print(f"Mapping: {mapping_path}")

    if ok > 0:
        print("\nNOTE: Restart the Uderia server or reload RAG collections for the new")
        print("      repositories to appear in the UI and be available for retrieval.")

    if ok < len(mapping):
        sys.exit(1)


if __name__ == "__main__":
    main()
